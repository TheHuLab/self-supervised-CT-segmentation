\section{Experiments}



\iffalse
\begin{table*}[!h]
	\centering
	\small
	\begin{tabular}{| c | c || c c c c || c c c c |}
		\hline
		& &\multicolumn{4}{c||}{Ground-Glass Opacity} & \multicolumn{4}{c|}{Consolidation}\\ \cline{3-10}
		Methods & & F1 & IoU & Recall & Precision & F1 & IoU & Recall & Precision \\\hline
		SInfNet & Mean & 0.87 & 0.81 & 0.87 & 0.9 & 0.47 & 0.39 & 0.68 & 0.55\\ \cline{2-10}
		& Error & $\pm$0.041 & $\pm$0.049 & $\pm$0.043 & $\pm$0.042 & $\pm$0.102 & $\pm$0.093 & $\pm$0.103 & $\pm$0.111 \\ \hline \hline
		
		\vtop{\hbox{\strut SInfNet+}\hbox{\strut data aug(0.4)}} & Mean &0.86 & 0.81 & 0.87 & 0.91 & 0.58 & 0.47 & 0.64 & 0.74 \\ \cline{2-10}
		& Error & $\pm$0.048 & $\pm$0.058 & $\pm$0.052 & $\pm$0.04 & $\pm$0.096 & $\pm$0.092 & $\pm$0.108 & $\pm$0.095  \\ \hline \hline
		
		\vtop{\hbox{\strut SInfNet+}\hbox{\strut data aug(0.5)}} & Mean &0.86 & 0.81 & 0.88 & 0.9& 0.53 & 0.44 & 0.62 & 0.69   \\ \cline{2-10}
		& Error &$\pm$0.045 & $\pm$0.055 & $\pm$0.05 & $\pm$0.042 & $\pm$0.108 & $\pm$0.099 & $\pm$0.118 & $\pm$0.108  \\ \hline \hline

		SSInfNet & Mean & 0.86 & 0.8 & 0.87 & 0.89 & 0.55 & 0.46 & 0.67 & 0.68  \\ \cline{2-10}
		& Error & $\pm$0.041 & $\pm$0.051 & $\pm$0.044 & $\pm$0.039 & $\pm$0.106 & $\pm$0.101 & $\pm$0.116 & $\pm$0.108 \\ \hline \hline
		
		\vtop{\hbox{\strut SSInfNet+}\hbox{\strut data aug}}& Mean & 0.82 & 0.74 & 0.82 & 0.86 & 0.27 & 0.22 & 0.61 & 0.31   \\ \cline{2-10}
		& Error & $\pm$0.046 & $\pm$0.053 & $\pm$0.048 & $\pm$0.04 & $\pm$0.077 & $\pm$0.067 & $\pm$0.117 & $\pm$0.088\\ \hline \hline \hline
		
		
		& &\multicolumn{4}{c||}{Background} & \multicolumn{4}{c|}{Overall}\\ \cline{3-10}
		Methods & & F1 & IoU & Recall & Precision & F1 & IoU & Recall & Precision \\\hline
		SInfNet & Mean & 1.0 & 1.0 & 1.0 & 1.0 & 0.78 & 0.73 & 0.85 & 0.82 \\ \cline{2-10}
		& Error &$\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.047 & $\pm$0.048 & $\pm$0.049 & $\pm$0.051 \\ \hline \hline
		\vtop{\hbox{\strut SInfNet+}\hbox{\strut data aug(0.4)}}  & Mean &1.0 & 1.0 & 1.0 & 1.0 & 0.81 & 0.76 & 0.84 & 0.88  \\ \cline{2-10}
		& Error & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.048 & $\pm$0.05 & $\pm$0.053 & $\pm$0.045 \\ \hline \hline
		\vtop{\hbox{\strut SInfNet+}\hbox{\strut data aug(0.5)}}  & Mean & 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 0.75 & 0.83 & 0.86 \\ \cline{2-10}
		& Error & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.051 & $\pm$0.051 & $\pm$0.056 & $\pm$0.05 \\ \hline \hline
		SSInfNet & Mean &1.0 & 1.0 & 1.0 & 1.0&0.8 & 0.75 & 0.85 & 0.86 \\ \cline{2-10}
		& Error &$\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.049 & $\pm$0.05 & $\pm$0.054 & $\pm$0.049 \\ \hline \hline
		\vtop{\hbox{\strut SSInfNet+}\hbox{\strut data aug}} & Mean & 1.0 & 1.0 & 1.0 & 1.0 & 0.69 & 0.65 & 0.81 & 0.72 \\ \cline{2-10}
		& Error &$\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.0 & $\pm$0.041 & $\pm$0.04 & $\pm$0.055 & $\pm$0.043 \\ \hline \hline
		
	\end{tabular}
	\caption{Quantitative result of Ground-glass Opacities \& Consolidation on the test data set. Prior is obtained from the test set.}
	\label{tab:multi-strongprior}
\end{table*}
\fi


\subsection{Datasets}
The dataset that we will be using is an integrative resource of chest computed tomography images and clinical features of patients with COVID-19 pneumonia (ICTCF) \cite{ref23} which contains the severity score for each CT lung image and CT lung images from medical segmentation website \cite{ref26}. 

ICTCF contains 127 types of clinical features and laboratory-confirmed cases of COVID-19 from 1170 patients including the severity of the CT lung images. However, the ICTCF dataset does not contain the segmentation labels for the ground-glass opacities and the consolidation in the CT lung images. In total, there are 6654 of CT lung images in ICTCF dataset. Originally, there were 1521 patients. However, some of the patients are missing CT lung images. We remove these patients that are missing CT lung images. After preprocessing the patients, the dataset was left with 1338 patients that contain CT lung images. The dataset can be found here: http://ictcf.biocuckoo.cn/.  We will use these ICTCF CT lung images without the ground truth segmentation labels in combination with the MedSeg dataset to undergo self-supervised learning to predict image in-painting. 

As for the MedSeg dataset, they contain ground truth labels for the segmentation for ground-glass opacities and consolidation of the CT lung images. The total amount of CT lung images contain in MedSeg dataset is 932 CT lung images. We randomly assign the CT lung images into a training set, validation set, and testing set of which the training set contains 698 CT lung images, the validation set contains 114 CT lung images, and the testing set contains 117 CT lung images. 

The assignment of the dataset can be seen in \ref{tab:dataset}.

\begin{table}[!h]
	\centering
	\begin{tabular}{|c||c|c|c|c|} \hline
		Data split & Source & Segmented & Images & Patients \\\hline
		Training & \vtop{\hbox{\strut Med-Seg}\hbox{\strut ICTCF}}&
		\vtop{\hbox{\strut Yes}\hbox{\strut No}} & 
		\vtop{\hbox{\strut 698}\hbox{\strut 6654}}&
		\vtop{\hbox{\strut 39}\hbox{\strut 1338}}\\\hline
		Validation & Med-Seg & Yes & 114 & 35 \\\hline
		Testing & Med-Seg & Yes & 117 & 35 \\\hline
	\end{tabular}
	\caption{This table shows the data distribution between the datasets that we use to evaluate our model on. Med-Seg refers to the COVID-19 CT Segmentation data set and ICTCF refers to the ICTCF data set.}
	\label{tab:dataset}
\end{table}


\subsection{Experimental Settings}
During the self-supervised image inpainting stage, we train the network for 2000 epochs. The network is trained for the first 200 epochs before we train the coach network for 200 epochs which increases the complexity of the masks generated. After that, we alternate in between training the self-supervised image inpainting and the coach network with 100 epochs in between. For every alternating between the training of the self-supervised image inpainting and the coach network, we set the learning rate to 0.1 at the start of the epoch, we set the learning rate to 0.01 at 40th epoch, we set the learning rate to 0.001 at 80th epochs, and 0.0001 at the 90th epoch.  We use SGD as the optimizer for the self-supervised image inpainting.  We set the momentum to 0.9 and the weight decay to 0.0005. As for the optimizer for the coach network, we use Adam optimizer with a learning rate of 0.00001.

For the Single InfNet, we train the network for 500 epochs. We use Adam as the optimizer with a learning rate of 0.0001. 

For the Multi InfNet, we train the network for 500 epochs. We use SGD as the optimizer. The momentum is set as 0.7 and the learning rate is set as 0.01.

