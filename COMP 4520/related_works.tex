\section{Related works}

There are several works that have been proposed to create image segmentation for CT scan lung images of COVID-19 positive patients. They have demonstrated effective solutions using deep neural networks to accurately predict if a patient has COVID-19 positive or negative. 

A study has been conducted that uses multiple models for different tasks where the study uses both classification and image segmentation tasks for COVID-19 detection through multi-tasks learning. The study uses Inception Residual Recurrent Neural Network (IRRCNN) for the classification of COVID-19 detection and uses Nabla-Net (NABLA-N) network for infected region segmentation for X-ray and CT images scan. \cite{ref1} Transfer learning is used to retrain the IRRCNN model with samples to differentiate between COVID-19 positive samples and negative samples in the classification phase. Mathematical Morphological approaches are implemented for selecting appropriate contours for chest region selection in the segmentation phase with NABLA-N network. Some classical imaging and adaptive threshold approaches are applied to extract the features to identify infected regions of COVID-19. They used a total number of 5,216 samples of which 3,875 samples are pneumonia and 1,341 samples are normal.

Another study \cite{ref2} introduces a feature variation block and progressive atrious spatial pyramid pooling block using COVID-segNet, a high accuracy network that is able to create segmentation of COVID-19 infection from chest CT images. The network consists of an Encoder and a Decoder with residual skip connection connecting the encoder and the decoder at their respective layer, following the architecture of UNET \cite{ref3}. Their main findings include the introduction of an FV block and a PASPP block. FV block consists of three branches - contrast enhancement branch, position sensitive branch, and identity branch. These branches can enable automatic change of parameters to display positions and boundaries of COVID-19. The PASPP block takes features extracted from the FV block to acquire semantic information with a variety of receptive fields. The dataset that they used consists of 21,658 labeled chest CT images, of which 861 CT images are confirmed COVID-19. 

Both the papers above however conducted the study with a good amount of data samples to train the network to achieve a high performance. The papers are only able to recognize the presence of COVID-19 in a patient, but the papers could not quantify the severity of the disease. 

While there is a limited number of public data samples available for CT COVID-19 lung images,  it will not be feasible to train a network to achieve high performance. There are a different number of research that resolve this issue. One method is to use self-supervised learning to mitigate the problem of having a low number of data samples to improve the performance of deep neural networks. Instead of having to manually annotate the data, self-supervised learning instead finds and exploits the relationship between data samples.

Novosel et al. \cite{ref8} used elf-supervised learning to boost the performance of a supervised pixel-level segmentation task. They used image colorization and depth prediction to improve the feature representation from the encoder network to improve the performance of the network by learning multi-tasks of the image domain. They show that self-supervised learning by training the network on semantic segmentation, image colorization and depth prediction instead of solely training on semantic segmentation improves the performance of the network.

Another study \cite{ref9} uses fine-grained segmentation networks (FGSN) to produce a dense segmentation map. Instead of using manually annotated labels, they created labels in a self-supervised manner. Features were extracted in certain layers in the network and clustered using k-means clustering. The cluster assignments are then used as supervision for the training. They showed that the learned representations can contain useful information for visual localization. The performance of the model improved by as much as 15% when trained with self-supervised learning instead of just classification.
